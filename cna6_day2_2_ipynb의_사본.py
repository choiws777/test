# -*- coding: utf-8 -*-
"""CNA6_day2-2.ipynb의 사본

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nW5ISi144eeVg1O-6pkkGmKlvitWuMyC

---

# <center>멤버십 고객 탈회 예측</center>

컬럼 | 설명 | 컬럼 | 설명
---|---|---|---
id | 멤버십 번호 | MEMBERSHIP_TERM_YEARS | 멤버십 기간
ANNUAL_FEES | 연간 회비 | MEMBER_MARITAL_STATUS | 기혼 여부, M(기혼), S(미혼)
MEMBER_GENDER | 성별, M(남),F(여) | MEMBER_ANNUAL_INCOME | 연봉(연간 수입)
MEMBER_OCCUPATION_CD | 직업 | MEMBERSHIP_PACKAGE | 멤버십 유형
MEMBER_AGE_AT_ISSUE | 나이 | ADDITIONAL_MEMBERS | 가족 회원의 수
PAYMENT_MODE | 지불 방식 | AGENT_CODE | 직원 코드 
START_DATE (YYYYMMDD) | 시작 날짜 | END_DATE  (YYYYMMDD) | 종료 날짜 
MEMBERSHIP_STATUS | 0(유지), 1(탈회) |
"""

# 이 과제에서는 F1 스코어를 쓸거니 잘 보삼

"""# Import Library"""

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings(action='ignore')

"""# 데이터셋 업로드"""

# 구글 드라이브 연결하기
from google.colab import drive
drive.mount('./MyDrive')

"""# Load Dataset"""

train = pd.read_csv("/content/MyDrive/MyDrive/motor_temperature-20211118T021215Z-001/motor_temperature/train.csv")
print(train.shape)
train.head()

test = pd.read_csv("/content/MyDrive/MyDrive/DB/DB_CNA6_실습파일/membership/test.csv")
print(test.shape)
test.head()

submission = pd.read_csv("/content/MyDrive/MyDrive/DB/DB_CNA6_실습파일/membership/sample_submission.csv")
submission.head()

"""# 데이터 탐색 (EDA)

### 데이터프레임 기본 정보
"""

train.info()

"""### 통계정보 요약"""

train.describe()

"""### 중복행 개수"""

train.duplicated().sum()

test.duplicated().sum()

"""### 결측치 개수"""

train.isnull().sum()

test.isnull().sum()

"""# Preprocessing

## 컬럼명을 간결하게 바꾸기
"""

train.columns

train.columns = ['id', 'term', 'fee', 'married', 'gender', 'income', 'job',
'package', 'age', 'family', 'payment', 'agent', 'target', 'stard_date', 'end_date']

test.columns

test.columns = ['id', 'term', 'fee', 'married', 'gender', 'income', 'job',
'package', 'age', 'family', 'payment', 'agent', 'stard_date']

"""## 결측값 처리"""

train.isnull().sum()

test.isnull().sum()

"""### 결측값 제거"""

# end_date 열 삭제
train.drop(['end_date'], axis=1)

train = train.drop(['end_date'], axis=1)

"""### 평균값 대체"""

# income
train['income'].mean()

income_mean = train['income'].mean()
train['income'] = train['income'].fillna(income_mean)
test['income'] = test['income'].fillna(income_mean)

sns.displot(x='income', hue='target', data=train, palette="PRGn");

"""### 최빈값 대체"""

# gender
train['gender'].mode()[0]

train['gender'] = train['gender'].fillna(train['gender'].mode()[0])
test['gender'] = test['gender'].fillna(train['gender'].mode()[0])

sns.countplot(x='gender', hue='target', data=train);

"""### 결측값을 별도로 표시"""

# married
train['married'].value_counts(dropna=False)

train['married'] = train['married'].fillna('U')
test['married'] = test['married'].fillna('U')

sns.countplot(x='married', hue='target', data=train);

"""# [실습]
job 컬럼에 속하는 결측값을 적절하게 처리합니다. 
"""

# job
train['job'].value_counts(dropna=False)

train['job'] = train['job'].fillna(train['job'].mode()[0])
test['job'] = test['job'].fillna(train['job'].mode()[0])



"""## Label Encoding"""

# 범주형 변수
categorical_features = []
for col in train.columns:
    if train[col].dtype == 'O':
        categorical_features.append(col)
        
categorical_features

"""### gender"""

train['gender'].value_counts()

# Label Encoding
train.loc[train['gender']=='M', 'gender'] = 0
train.loc[train['gender']=='F', 'gender'] = 1

test.loc[test['gender']=='M', 'gender'] = 0
test.loc[test['gender']=='F', 'gender'] = 1

train['gender'].value_counts()

"""### married"""

train['married'].value_counts(dropna=False)

# Label Encoder 활용
from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
encoder.fit(train['married'])
train['married'] = encoder.transform(train['married'])
test['married'] = encoder.transform(test['married'])

train['married'].value_counts()

"""## [Quiz]
package 변수의 범주형 자료를 Label Encoding 기법을 적용하여 숫자로 변환합니다. 
"""

train['package'].value_counts()

# Label Encoding
train.loc[train['package']=='TYPE-A', 'package'] = 0
train.loc[train['package']=='TYPE-B', 'package'] = 1

test.loc[test['package']=='TYPE-A', 'package'] = 0
test.loc[test['package']=='TYPE-B', 'package'] = 1

train['package'].value_counts()



"""## One-hot Encoding"""

# One-Hot Encoding

pd.get_dummies(train, columns = ['payment'], drop_first=False)

pd.get_dummies(train, columns = ['payment'], drop_first=True)

# train, test 결합 
data = train.append(test)
data

data = pd.get_dummies(data, columns = ['payment'], drop_first=True)
data

new_train = data[data['target'].notnull()]
new_test = data[data['target'].isnull()]

new_train.shape, new_test.shape

new_test



"""## Binning"""

# Age 분포
sns.displot(x='age', kind='hist', data=train);

age_bins = [0, 20, 30, 40, 50, 60, 70, 150]

age_labels = ['10-', '20s', '30s', '40s', '50s', '60s', '70+']

pd.cut(train['age'], bins=age_bins, labels=age_labels)

train['age_bin'] = pd.cut(train['age'], bins=age_bins, labels=age_labels)
train['age_bin'].value_counts()

sns.countplot(x='age_bin', hue='target', data=train);

test['age_bin'] = pd.cut(test['age'], bins=age_bins, labels=age_labels)
test['age_bin'].value_counts()

sns.displot(x='age_bin', kind='hist', hue='target', data=train);

"""## Feature interaction"""

train["gender"].value_counts()

train["gender"].astype(str)

train["age_bin"].astype(str) + '_' + train["gender"].astype(str)

# Age_Bin과 Gender 결합
train['age_gender'] = train["age_bin"].astype(str) + '_' + train["gender"].astype(str)
train['age_gender'].value_counts()

sns.displot(x='age_gender', kind='hist', hue='target', data=train)
plt.xticks(rotation=90)
plt.show()

test['age_gender'] = test["age_bin"].astype(str) + '_' + test["gender"].astype(str)
test['age_gender'].value_counts()

"""## 로그 변환"""

# Annual_Premium 분포
sns.displot(x='fee', kind='kde', hue='target', data=train);

# log 변환
train['fee_log'] = np.log1p(train['fee'])
train['fee_log']

sns.displot(x='fee_log', kind='kde', hue='target', data=train);

test['fee_log'] = np.log1p(test['fee'])
sns.displot(x='fee_log', kind='kde', data=test);

"""# Baseline 모델"""

train.head()

"""## Feature 선택"""

# 숫자형 변수 선택
for col in train.columns:
    if not train[col].dtype=='object':
        print(col)

print([col for col in train.columns[1:] if not train[col].dtype=='object'])

"""## X, y 변수 정리"""

selected_features = ['term', 'fee', 'married', 'income', 'job', 'age', 'family']

X_train = train.loc[:, selected_features]
X_test = test.loc[:, selected_features]

y_train = train.loc[:, 'target']

print(X_train.shape, y_train.shape)
print(X_test.shape)

"""## 결측치 처리"""

X_train.isnull().sum()

X_test.isnull().sum()

"""## Train-Test 데이터셋 분할"""

from sklearn.model_selection import train_test_split
X_tr, X_val, y_tr, y_val =  train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=2021)

print("훈련 데이터셋: ", X_tr.shape, y_tr.shape)
print("검증 데이터셋: ", X_val.shape, y_val.shape)

X_train.isnull().sum().sum(), X_test.isnull().sum().sum()

"""## 분류 알고리즘 선택"""

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(random_state=2021)

"""## 모델 학습"""

model.fit(X_tr, y_tr)

"""## 모델 예측"""

val_pred = model.predict(X_val)
val_pred[:10]

y_val[:10].to_list()

"""## 성능 평가

성능 평가 지표는 **f1 score**를 사용하였습니다.  
f1 score는 Precision과 Recall의 조화평균으로 주로 **분류 클래스 간의 데이터가 불균형이 심각할때 사용**합니다.  
<img src='https://t1.daumcdn.net/cfile/tistory/99DC064C5BE056CE10' width=50%>
<img src='https://miro.medium.com/max/700/1*3KDYxZCMmGbUDtmQdnYhmw.jpeg' width=50%>
<img src='https://miro.medium.com/max/1400/1*A0Lu2dZfWsCMqWlhw1ZNfQ.png' width=70%>
"""

# Confusion Matrix
from sklearn.metrics import confusion_matrix 
from sklearn.metrics import plot_confusion_matrix

plot_confusion_matrix(model, X_val, y_val,
                      values_format='d', cmap='cividis', 
                      display_labels=["1", "0"])

# 평가 지표 - Accuracy
from sklearn.metrics import accuracy_score

print("정확도: %0.2f" % accuracy_score(y_val, val_pred))

# 평가 지표 - Precision
from sklearn.metrics import precision_score

print("정밀도: %0.2f" % precision_score(y_val, val_pred))

# 평가 지표 - Recall
from sklearn.metrics import recall_score

print("재현율: %0.2f" % recall_score(y_val, val_pred))

# 평가 지표 - F1 score
from sklearn.metrics import f1_score

print("F1 스코어: %0.2f" % f1_score(y_val, val_pred))

# 평가 지표 - ROC AUC
from sklearn.metrics import roc_auc_score

print("ROC AUC: %0.2f" % roc_auc_score(y_val, val_pred))

"""## Submission 파일 만들기"""

submission = pd.read_csv('/content/MyDrive/MyDrive/DB/DB_CNA6_실습파일/membership/sample_submission.csv')
submission

preds = model.predict(X_test)
submission['MEMBERSHIP_STATUS'] = preds
submission

submission['MEMBERSHIP_STATUS'].value_counts() / submission.shape[0]

submission.to_csv('prediction_001.csv', index=False)

"""# Feature Engineering

## Feature Importance
"""

feature_names = X_train.columns
feature_names

importances = model.feature_importances_
importances

plt.figure(figsize=(12, 4))
sns.barplot(x=feature_names, y=importances, estimator=np.mean)
plt.title("Feature importances")
plt.ylabel("Mean decrease in impurity")
plt.show()

"""## Feature Scaling"""

X_train.describe()

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()

X_train_scaled = X_train.copy()

scaler.fit(X_train)
X_train_scaled.loc[:, :] = scaler.transform(X_train)

X_train_scaled.head(2)

X_test_scaled = X_test.copy()
X_test_scaled.loc[:, :] = scaler.transform(X_test)

X_test_scaled.head(2)

X_tr, X_val, y_tr, y_val =  train_test_split(X_train_scaled, y_train, test_size=0.2, stratify=y_train, random_state=2021)

print("훈련 데이터셋: ", X_tr.shape, y_tr.shape)
print("검증 데이터셋: ", X_val.shape, y_val.shape)

model = RandomForestClassifier(random_state=2021)
model.fit(X_tr, y_tr)
val_pred = model.predict(X_val)

# 평가 지표
print("정확도: %0.2f" % accuracy_score(y_val, val_pred))
print("정밀도: %0.2f" % precision_score(y_val, val_pred))
print("재현율: %0.2f" % recall_score(y_val, val_pred))
print("F1 스코어: %0.2f" % f1_score(y_val, val_pred))
print("ROC AUC: %0.2f" % roc_auc_score(y_val, val_pred))

plt.figure(figsize=(12, 4))
sns.barplot(x=X_train.columns, y=model.feature_importances_, estimator=np.mean)
plt.title("Feature importances")
plt.ylabel("Mean decrease in impurity")
plt.show()

preds = model.predict(X_test_scaled)
submission['MEMBERSHIP_STATUS'] = preds
submission.to_csv('prediction_002.csv', index=False)
submission

submission['MEMBERSHIP_STATUS'].value_counts() / submission.shape[0]

"""# 모델 성능 개선하기

## 변수 선택 
Feature Importance 분석에 따라, 가장 낮는 married, job 열을 제외하고 분석
"""

X_train_scaled.head(2)

X_train_scaled.columns

selected_features = ['term', 'fee', 'income', 'age', 'family']
X_train_scaled = X_train_scaled.loc[:, selected_features]
X_test_scaled = X_test_scaled.loc[:, selected_features]

X_tr, X_val, y_tr, y_val =  train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=2021)
print("훈련 데이터셋: ", X_tr.shape, y_tr.shape)
print("검증 데이터셋: ", X_val.shape, y_val.shape)

model = RandomForestClassifier(random_state=2021)
model.fit(X_tr, y_tr)
val_pred = model.predict(X_val)

# 평가 지표
print("정확도: %0.2f" % accuracy_score(y_val, val_pred))
print("정밀도: %0.2f" % precision_score(y_val, val_pred))
print("재현율: %0.2f" % recall_score(y_val, val_pred))
print("F1 스코어: %0.2f" % f1_score(y_val, val_pred))
print("ROC AUC: %0.2f" % roc_auc_score(y_val, val_pred))

plt.figure(figsize=(12, 4))
sns.barplot(x=X_train_scaled.columns, y=model.feature_importances_, estimator=np.mean)
plt.title("Feature importances")
plt.ylabel("Mean decrease in impurity")
plt.show()

"""## 모델 파라미터 변경"""

model = RandomForestClassifier(n_estimators=300, class_weight='balanced', random_state=2021)
model.fit(X_tr, y_tr)
val_pred = model.predict(X_val)

# 평가 지표
print("정확도: %0.2f" % accuracy_score(y_val, val_pred))
print("정밀도: %0.2f" % precision_score(y_val, val_pred))
print("재현율: %0.2f" % recall_score(y_val, val_pred))
print("F1 스코어: %0.2f" % f1_score(y_val, val_pred))
print("ROC AUC: %0.2f" % roc_auc_score(y_val, val_pred))

plt.figure(figsize=(12, 4))
sns.barplot(x=X_train_scaled.columns, y=model.feature_importances_, estimator=np.mean)
plt.title("Feature importances")
plt.ylabel("Mean decrease in impurity")
plt.show()

"""## SMOTE 업샘플링"""

# !pip install imblearn

from imblearn.over_sampling import SMOTE
smote = SMOTE()
len_train = len(X_train)
X_tr_smote, y_tr_smote = smote.fit_resample(X_tr, y_tr)

X_tr_smote.shape, X_tr.shape

model = RandomForestClassifier(n_estimators=300, class_weight='balanced', random_state=2021)
model.fit(X_tr_smote, y_tr_smote)
val_pred = model.predict(X_val)

# 평가 지표
print("정확도: %0.2f" % accuracy_score(y_val, val_pred))
print("정밀도: %0.2f" % precision_score(y_val, val_pred))
print("재현율: %0.2f" % recall_score(y_val, val_pred))
print("F1 스코어: %0.2f" % f1_score(y_val, val_pred))
print("ROC AUC: %0.2f" % roc_auc_score(y_val, val_pred))

plt.figure(figsize=(12, 4))
sns.barplot(x=X_train_scaled.columns, y=model.feature_importances_, estimator=np.mean)
plt.title("Feature importances")
plt.ylabel("Mean decrease in impurity")
plt.show()

"""## One-Hot Encoding"""

# Vehicle_Damage 
pd.get_dummies(X_train, columns = ['married'], drop_first=True)

X_data = X_train.append(X_test)
X_data = pd.get_dummies(X_data, columns = ['married', 'job'], drop_first=True)
X_data.head()

X_train = X_data.iloc[:len(X_train)]
X_test = X_data.iloc[len(X_train):]
X_test

# stratify
X_tr, X_val, y_tr, y_val =  train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=2021)
print("훈련 데이터셋: ", X_tr.shape, y_tr.shape)
print("검증 데이터셋: ", X_val.shape, y_val.shape)

model = RandomForestClassifier(n_estimators=300, class_weight='balanced', random_state=2021)
model.fit(X_tr, y_tr)
val_pred = model.predict(X_val)

# 평가 지표
print("정확도: %0.2f" % accuracy_score(y_val, val_pred))
print("정밀도: %0.2f" % precision_score(y_val, val_pred))
print("재현율: %0.2f" % recall_score(y_val, val_pred))
print("F1 스코어: %0.2f" % f1_score(y_val, val_pred))
print("ROC AUC: %0.2f" % roc_auc_score(y_val, val_pred))

plt.figure(figsize=(12, 4))
sns.barplot(x=X_train.columns, y=model.feature_importances_, estimator=np.mean)
plt.title("Feature importances")
plt.ylabel("Mean decrease in impurity")
plt.show()

"""# 다른 알고리즘 적용

### XGBoost
"""

import xgboost as xgb

model2 = xgb.XGBClassifier(n_estimators=300, scale_pos_weight=10, random_state=2021)  # scale_pos_weight = total_majority_examples / total_minority_examples
model2.fit(X_tr, y_tr)
val_pred2 = model2.predict(X_val)

# 평가 지표
print("정확도: %0.2f" % accuracy_score(y_val, val_pred2))
print("정밀도: %0.2f" % precision_score(y_val, val_pred2))
print("재현율: %0.2f" % recall_score(y_val, val_pred2))
print("F1 스코어: %0.2f" % f1_score(y_val, val_pred2))
print("ROC AUC: %0.2f" % roc_auc_score(y_val, val_pred2))

plt.figure(figsize=(12, 4))
sns.barplot(x=X_train.columns, y=model2.feature_importances_, estimator=np.mean)
plt.title("Feature importances")
plt.ylabel("Mean decrease in impurity")
plt.show()

"""### LightGBM"""

import lightgbm as lgb
model3 = lgb.LGBMClassifier(n_estimators=300, scale_pos_weight=10, random_state=2021)
model3.fit(X_tr, y_tr)
val_pred3 = model3.predict(X_val)

# 평가 지표
print("정확도: %0.2f" % accuracy_score(y_val, val_pred3))
print("정밀도: %0.2f" % precision_score(y_val, val_pred3))
print("재현율: %0.2f" % recall_score(y_val, val_pred3))
print("F1 스코어: %0.2f" % f1_score(y_val, val_pred3))
print("ROC AUC: %0.2f" % roc_auc_score(y_val, val_pred3))

plt.figure(figsize=(12, 4))
sns.barplot(x=X_train.columns, y=model3.feature_importances_, estimator=np.mean)
plt.title("Feature importances")
plt.ylabel("Mean decrease in impurity")
plt.show()

"""# K-fold 교차검증"""

from sklearn.model_selection import StratifiedKFold

cv_score = []
cv_model = lgb.LGBMClassifier(n_estimators=300, scale_pos_weight=10, random_state=2021)

idx_iter = 0
for train_idx , valid_idx in StratifiedKFold(n_splits=5, random_state=42, shuffle=True).split(X_train, y_train):  

    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[valid_idx]
    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[valid_idx]

    #학습 및 예측 
    cv_model.fit(X_tr , y_tr)    
    y_pred = cv_model.predict(X_val)

    # 반복 시 마다 정확도 측정 
    idx_iter += 1
    score = f1_score(y_val, y_pred)

    print(f"{idx_iter}번째 Valid Fold의 F1 Score: {score}")

    cv_score.append(score)

# 평균 스코어
np.mean(cv_score)

"""# [실습]
고객 이탈 여부를 예측하는 모델을 훈련시키고 예측 결과를 시스템에 제출합니다. (합격 점수: F1스코어 0.52 이상)
"""

